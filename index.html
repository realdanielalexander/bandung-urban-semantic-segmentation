<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Application of Convolutional Neural Network for Semantic Segmentation of Bandung Urban Scenes</title>
  <link
  rel="icon"
  href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ‘€</text></svg>"
  ></link>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Application of Convolutional Neural Network for Semantic Segmentation of Bandung Urban Scenes</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.realdanielalexander.com/" target="_blank">Daniel Alexander</a>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=s51dQZYAAAAJ&hl=en" target="_blank">Hans Christian Kurniawan</a>,</span>
                  <span class="author-block">
                    <a href="https://sinta.kemdikbud.go.id/authors/profile/6782192" target="_blank">Irfin Afifudin</a>,
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=hxqXZbMAAAAJ&hl=en" target="_blank">Hery Heryanto</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Harapan Bangsa Institute of Technology<br>2022
                      International Conference on Data and Software Engineering (ICoDSE)</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://doi.org/10.1109/ICoDSE56892.2022.9972006" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop ">
    <div class="hero-body">
      <img src="static/images/overview.gif" alt="Overview"/>
      <h2 class="subtitle has-text-centered">
        The Bandung Cityscapes dataset used as the object of this study
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Semantic segmentation is the process of classifying each pixel in an image into one of the predesignated classes. In its application for self-driving systems, semantic segmentation is used to identify objects and road conditions to make the right control decisions. This work uses the convolutional neural network method to perform semantic segmentation with the DeepLabV3+ architecture. Object of study is the urban scene dataset called Bandung Cityscapes collected around the city of Bandung, West Java, Indonesia and annotated by hand. Architecture testing is performed by combining the values of epoch, learning rate, and number of convolution filters to find the best mean intersection over union accuracy. The most optimal model achieved the highest average accuracy of 77.431% on the validation set, attained by the model with 128 convolutional filters, trained with 300 epochs and 0.0001 (10<sup>-4</sup>) learning rate. This model maintained an average training accuracy of 87.194%, average validation accuracy of 75.274%, and average difference between the two of 11.919% throughout the training process.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<br/>
<br/>

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Pipeline</h2>
      <div class="item" style="text-align: center;">
        <!-- Your image here -->
        <img src="static/images/framework.png" alt="Framework" class="img-framework" width="600"/>
        <h2 class="subtitle has-text-justified">
          <b>Framework pipeline</b>: (1) First, images are loaded with the corresponding ground truth labels. (2) Pre-processing is done with image data augmentation techniques, namely horizontal flip and gaussian blur, followed by label conversion into a one-hot encoded form. Images and labels are then passed through CNN in the training phase, initialized using He Initializer and the weights of each kernel is updated using Adam optimizer and the focal loss function. (3) evaluation is performed which will produce a prediction class for each pixel. These predictions are then compared with the corresponding labels to calculate mean intersection over union accuracy. 
        </h2>
      </div>
      <br/>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/arch-64.png" alt="Architecture 64" class="img-arch"/>
        <h2 class="subtitle has-text-centered">
          <b>64 convolutional filters architecture.</b>
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/arch-128.png" alt="Architecture 128" class="img-arch"/>
        <h2 class="subtitle has-text-centered">
          <b>128 convolutional filters architecture.</b>
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/arch-256.png" alt="Architecture 256" class="img-arch"/>
        <h2 class="subtitle has-text-centered">
          <b>256 convolutional filters architecture.</b>
       </h2>
     </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<br/>
<br/>

<!-- Youtube video -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <!-- Paper video. -->
      <h2 class="title is-3">Results</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <table>
            <thead>
              <tr>
                <th>Architecture</th>
                <th>Average prediction accuracy (Mean IoU)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>64 Filters</td>
                <td>75.893%</td>
              </tr>
              <tr>
                <td>128 Filters</td>
                <td>77.431%</td>
              </tr>
              <tr>
                <td>256 Filters</td>
                <td>73.942%</td>
              </tr>
            </tbody>
          </table>
          <h2 class="subtitle has-text-centered">
            <b>Average prediction accuracies on the validation set</b>
          </h2>

          <div class="item" style="text-align: center;">
            <!-- Your image here -->
            <img src="static/images/qualitative.png" alt="Framework" width="1000"/>
            <h2 class="subtitle has-text-justified">
              <b>Qualitative test results on validation data for each architecture</b>: from left to right: validation image, validation label, 64 filters prediction, 128 filters prediction, 256 filters prediction.
            </h2>
          </div>
          <br/>
          <br/>
          <div class="item" style="text-align: center;">
            <!-- Your image here -->
            <img src="static/images/mockups.png" alt="Framework" width="1000"/>
            <h2 class="subtitle has-text-justified">
              <b>Mobile application mockup</b>: To better facilitate the prediction process, a web/mobile application is developed using Flutter that sends an Application Programming Interface (API) call to the server de- veloped using Flask. 
            </h2>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop content">
      <h2 class="title is-3">Conclusion</h2>
      <p>The proposed solution successfully performed semantic segmentation on the Bandung Cityscapes dataset with the most optimal model achieving the highest average accuracy of 77.431% on the validation set attained by the model with 128 convolutional filters, trained with 300 epochs and 0.0001 (10-4) learning rate. This model maintained an average training accuracy of 87.194%, average validation accuracy of 75.274%, and average difference between the two of 11.919% throughout the training process. However, this model still has a difference between training and validation accuracy which point to the risk of overfitting. The combination of epoch, learning, rate and the number of convolutional filters greatly affects the performance of the model. For real world usage, there needs to be an investigation regarding effective range and how detailed the predictions need to be in order to determine the best model.</p>
    </div>
  </div>
</section>
<!-- End video carousel -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@INPROCEEDINGS{9972006,
        author={Alexander, Daniel and Kurniawan, Hans Christian and Afifudin, Irfin and Heryanto, Hery},
        booktitle={2022 International Conference on Data and Software Engineering (ICoDSE)}, 
        title={Application of Convolutional Neural Network for Semantic Segmentation of Bandung Urban Scenes}, 
        year={2022},
        volume={},
        number={},
        pages={12-17},
        keywords={Training;Semantic segmentation;Computational modeling;Roads;Urban areas;Predictive models;Convolutional neural networks;computer vision;semantic segmentation;urban scenes;Convolutional Neural Network (CNN);DeepLabV3+},
        doi={10.1109/ICoDSE56892.2022.9972006}}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
